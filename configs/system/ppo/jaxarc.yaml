system_name: ff_ppo # Name of the system.

# Learning rates - PHASE 1: Increase critic LR to help value function catch up
actor_lr: 3e-4
critic_lr: 1e-4 # CHANGED: 3e-5 → 1e-4 (faster value learning)

# Data collection - PHASE 2: More epochs and larger minibatches for stability
rollout_length: 256
epochs: 8 # CHANGED: 4 → 8 (more optimization per rollout)
num_minibatches: 4 # CHANGED: 8 → 4 (larger minibatches = more stable gradients)

# Discounting - PHASE 1: Lower gamma to reduce long-horizon credit assignment
gamma: 0.98 # CHANGED: 0.99 → 0.98 (shorter effective horizon)
gae_lambda: 0.92

# PPO parameters - PHASE 1/2: More exploration, stronger value learning
clip_eps: 0.2 # REVERTED: 0.3 → 0.2 (standard PPO, more conservative)
ent_coef: 0.05 # CHANGED: 0.03 → 0.05 (slower entropy decay)
vf_coef: 2.0 # CHANGED: 1.0 → 2.0 (emphasize value learning more)
max_grad_norm: 0.5

# Learning rate decay - PHASE 1: Disable to maintain exploration
decay_learning_rates: false # CHANGED: true → false (constant LR for now)

# Reward and observation processing - BALANCED: Moderate reward scaling
reward_scale: 0.08 # CHANGED: 0.05 → 0.08 (restore learning signal while maintaining stability)
standardize_advantages: true # KEEP ON
normalize_observations: false
obs_norm_warmup_steps: 128

# KL penalty (if using PPO-Penalty variant)
kl_penalty_coef: 3.0
