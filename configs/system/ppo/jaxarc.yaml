system_name: ff_ppo # Name of the system.

# Learning rates - keep as is initially
actor_lr: 3e-4
critic_lr: 3e-4

# Data collection - increase for better statistics
rollout_length: 256 # CHANGED: 128 → 256
epochs: 4
num_minibatches: 8 # CHANGED: 16 → 8 (larger minibatches)

# Discounting
gamma: 0.99
gae_lambda: 0.92 # CHANGED: 0.95 → 0.90 (less variance)

# PPO parameters - adjust for high variance
clip_eps: 0.3 # CHANGED: 0.2 → 0.3 (allow larger updates)
ent_coef: 0.03 # CHANGED: 0.01 → 0.02 (more exploration)
vf_coef: 1.0 # CHANGED: 0.5 → 1.0 (emphasize value learning)
max_grad_norm: 0.5

# Learning rate decay
decay_learning_rates: True

# Reward and observation processing - CRITICAL
reward_scale: 0.15 # CHANGED: 1.0 → 0.1 (compress rewards)
standardize_advantages: True # KEEP ON
normalize_observations: False
obs_norm_warmup_steps: 128

# KL penalty (if using PPO-Penalty variant)
kl_penalty_coef: 3.0
